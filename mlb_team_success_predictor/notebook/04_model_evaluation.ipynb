{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# MLB Team Success Predictor - Model Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook performs comprehensive evaluation of the trained models.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Objectives:\\n\",\n",
    "    \"1. Load and evaluate saved models\\n\",\n",
    "    \"2. Generate detailed performance metrics\\n\",\n",
    "    \"3. Create visualization reports\\n\",\n",
    "    \"4. Analyze prediction errors\\n\",\n",
    "    \"5. Compare model performance\\n\",\n",
    "    \"6. Test on historical data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add project root to path\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import custom modules\\n\",\n",
    "    \"from src.evaluation.metrics import ClassificationMetrics, RegressionMetrics\\n\",\n",
    "    \"from src.evaluation.model_evaluation import ClassificationEvaluator, RegressionEvaluator\\n\",\n",
    "    \"from src.visualization.model_plots import ModelVisualizer\\n\",\n",
    "    \"from src.prediction.predictor import DivisionWinnerPredictor, WinsPredictor\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries loaded successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Models and Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load saved models\\n\",\n",
    "    \"division_predictor = DivisionWinnerPredictor()\\n\",\n",
    "    \"division_predictor.load_model()\\n\",\n",
    "    \"division_predictor.load_scaler()\\n\",\n",
    "    \"\\n\",\n",
    "    \"wins_predictor = WinsPredictor()\\n\",\n",
    "    \"wins_predictor.load_model()\\n\",\n",
    "    \"wins_predictor.load_scaler()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Models loaded successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load test data\\n\",\n",
    "    \"data_path = Path('../data/processed/mlb_data_engineered.csv')\\n\",\n",
    "    \"df = pd.read_csv(data_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load feature lists\\n\",\n",
    "    \"with open('../data/processed/feature_lists.json', 'r') as f:\\n\",\n",
    "    \"    feature_lists = json.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter to test years (2022-2024)\\n\",\n",
    "    \"test_df = df[df['year'] >= 2022].copy()\\n\",\n",
    "    \"print(f\\\"Test data shape: {test_df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Test years: {test_df['year'].unique()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Division Winner Classification Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare classification test data\\n\",\n",
    "    \"class_features = feature_lists['classification_features']\\n\",\n",
    "    \"class_test_df = test_df[class_features + ['is_division_winner', 'team_name', 'year']].dropna()\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_test_class = class_test_df[class_features]\\n\",\n",
    "    \"y_test_class = class_test_df['is_division_winner']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Classification test samples: {len(X_test_class)}\\\")\\n\",\n",
    "    \"print(f\\\"Division winners: {y_test_class.sum()} ({y_test_class.mean():.1%})\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Make predictions\\n\",\n",
    "    \"class_predictions = division_predictor.predict_with_confidence(X_test_class)\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_pred = np.array(class_predictions['predictions'])\\n\",\n",
    "    \"y_proba = np.array(class_predictions['probabilities'])[:, 1] if len(class_predictions['probabilities'][0]) > 1 else np.array(class_predictions['probabilities'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate metrics\\n\",\n",
    "    \"class_metrics = ClassificationMetrics(y_test_class.values, y_pred, y_proba.reshape(-1, 1))\\n\",\n",
    "    \"metrics = class_metrics.get_metrics()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Classification Performance:\\\")\\n\",\n",
    "    \"print(f\\\"  Accuracy: {metrics['accuracy']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"  Precision: {metrics['precision']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"  Recall: {metrics['recall']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"  F1 Score: {metrics['f1']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"  ROC AUC: {metrics.get('roc_auc', 'N/A')}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Confusion Matrix\\n\",\n",
    "    \"visualizer = ModelVisualizer()\\n\",\n",
    "    \"fig = visualizer.plot_confusion_matrix_advanced(\\n\",\n",
    "    \"    y_test_class.values, y_pred,\\n\",\n",
    "    \"    labels=['Not Winner', 'Division Winner'],\\n\",\n",
    "    \"    model_name='Division Winner Classifier'\\n\",\n",
    "    \")\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze predictions by confidence level\\n\",\n",
    "    \"confidence_analysis = pd.DataFrame({\\n\",\n",
    "    \"    'team': class_test_df['team_name'].values,\\n\",\n",
    "    \"    'year': class_test_df['year'].values,\\n\",\n",
    "    \"    'actual': y_test_class.values,\\n\",\n",
    "    \"    'predicted': y_pred,\\n\",\n",
    "    \"    'probability': y_proba,\\n\",\n",
    "    \"    'confidence': class_predictions['confidence_scores'],\\n\",\n",
    "    \"    'confidence_level': class_predictions['confidence_levels']\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Accuracy by confidence level\\n\",\n",
    "    \"conf_accuracy = confidence_analysis.groupby('confidence_level').apply(\\n\",\n",
    "    \"    lambda x: (x['actual'] == x['predicted']).mean()\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nAccuracy by Confidence Level:\\\")\\n\",\n",
    "    \"print(conf_accuracy)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Win Total Regression Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare regression test data\\n\",\n",
    "    \"reg_features = feature_lists['regression_features']\\n\",\n",
    "    \"reg_test_df = test_df[reg_features + ['wins', 'team_name', 'year']].dropna()\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_test_reg = reg_test_df[reg_features]\\n\",\n",
    "    \"y_test_reg = reg_test_df['wins']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Regression test samples: {len(X_test_reg)}\\\")\\n\",\n",
    "    \"print(f\\\"Win distribution: mean={y_test_reg.mean():.1f}, std={y_test_reg.std():.1f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Make predictions\\n\",\n",
    "    \"reg_predictions = wins_predictor.predict_with_bounds(X_test_reg)\\n\",\n",
    "    \"\\n\",\n",
    "    \"y_pred_reg = np.array(reg_predictions['predictions'])\\n\",\n",
    "    \"lower_bounds = np.array(reg_predictions['lower_bounds'])\\n\",\n",
    "    \"upper_bounds = np.array(reg_predictions['upper_bounds'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate metrics\\n\",\n",
    "    \"reg_metrics = RegressionMetrics(y_test_reg.values, y_pred_reg)\\n\",\n",
    "    \"metrics_reg = reg_metrics.metrics\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Regression Performance:\\\")\\n\",\n",
    "    \"print(f\\\"  RMSE: {metrics_reg['rmse']:.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  MAE: {metrics_reg['mae']:.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  R²: {metrics_reg['r2']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"  Within 5 wins: {metrics_reg['within_5']:.1%}\\\")\\n\",\n",
    "    \"print(f\\\"  Within 10 wins: {metrics_reg['within_10']:.1%}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Regression diagnostics\\n\",\n",
    "    \"fig = visualizer.plot_regression_diagnostics(\\n\",\n",
    "    \"    y_test_reg.values, y_pred_reg,\\n\",\n",
    "    \"    model_name='Wins Predictor'\\n\",\n",
    "    \")\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze prediction errors\\n\",\n",
    "    \"reg_analysis = pd.DataFrame({\\n\",\n",
    "    \"    'team': reg_test_df['team_name'].values,\\n\",\n",
    "    \"    'year': reg_test_df['year'].values,\\n\",\n",
    "    \"    'actual_wins': y_test_reg.values,\\n\",\n",
    "    \"    'predicted_wins': y_pred_reg,\\n\",\n",
    "    \"    'error': y_pred_reg - y_test_reg.values,\\n\",\n",
    "    \"    'abs_error': np.abs(y_pred_reg - y_test_reg.values)\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Worst predictions\\n\",\n",
    "    \"print(\\\"\\\\nWorst Predictions (Largest Errors):\\\")\\n\",\n",
    "    \"worst_predictions = reg_analysis.nlargest(10, 'abs_error')\\n\",\n",
    "    \"print(worst_predictions[['team', 'year', 'actual_wins', 'predicted_wins', 'error']].round(1))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Model Performance by Year\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze performance by year\\n\",\n",
    "    \"yearly_performance = reg_analysis.groupby('year').agg({\\n\",\n",
    "    \"    'abs_error': ['mean', 'std'],\\n\",\n",
    "    \"    'error': 'mean'\\n\",\n",
    "    \"}).round(2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"yearly_performance.columns = ['MAE', 'Error_Std', 'Mean_Error']\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Performance by Year:\\\")\\n\",\n",
    "    \"print(yearly_performance)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize\\n\",\n",
    "    \"fig, ax = plt.subplots(figsize=(10, 6))\\n\",\n",
    "    \"years = yearly_performance.index\\n\",\n",
    "    \"ax.bar(years, yearly_performance['MAE'], yerr=yearly_performance['Error_Std'], \\n\",\n",
    "    \"       capsize=5, alpha=0.7)\\n\",\n",
    "    \"ax.set_xlabel('Year')\\n\",\n",
    "    \"ax.set_ylabel('Mean Absolute Error')\\n\",\n",
    "    \"ax.set_title('Model Performance by Year')\\n\",\n",
    "    \"ax.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract feature importance if available\\n\",\n",
    "    \"if hasattr(division_predictor.model, 'feature_importances_'):\\n\",\n",
    "    \"    importance_df = pd.DataFrame({\\n\",\n",
    "    \"        'feature': class_features,\\n\",\n",
    "    \"        'importance': division_predictor.model.feature_importances_\\n\",\n",
    "    \"    }).sort_values('importance', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot top features\\n\",\n",
    "    \"    plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"    top_n = 20\\n\",\n",
    "    \"    plt.barh(range(top_n), importance_df.head(top_n)['importance'])\\n\",\n",
    "    \"    plt.yticks(range(top_n), importance_df.head(top_n)['feature'])\\n\",\n",
    "    \"    plt.xlabel('Importance')\\n\",\n",
    "    \"    plt.title('Top 20 Features - Division Winner Prediction')\\n\",\n",
    "    \"    plt.gca().invert_yaxis()\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"\\\\nTop 10 Most Important Features:\\\")\\n\",\n",
    "    \"    print(importance_df.head(10))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Prediction Calibration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Calibration plot for classification\\n\",\n",
    "    \"from sklearn.calibration import calibration_curve\\n\",\n",
    "    \"\\n\",\n",
    "    \"fraction_of_positives, mean_predicted_value = calibration_curve(\\n\",\n",
    "    \"    y_test_class, y_proba, n_bins=10\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(8, 8))\\n\",\n",
    "    \"plt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Model')\\n\",\n",
    "    \"plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\\n\",\n",
    "    \"plt.xlabel('Mean Predicted Probability')\\n\",\n",
    "    \"plt.ylabel('Fraction of Positives')\\n\",\n",
    "    \"plt.title('Calibration Plot - Division Winner Classifier')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate calibration metrics\\n\",\n",
    "    \"if 'expected_calibration_error' in metrics:\\n\",\n",
    "    \"    print(f\\\"\\\\nExpected Calibration Error: {metrics['expected_calibration_error']:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"Max Calibration Error: {metrics.get('max_calibration_error', 'N/A')}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Historical Performance Test\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test on different eras\\n\",\n",
    "    \"eras_to_test = ['steroid', 'modern']\\n\",\n",
    "    \"era_results = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for era in eras_to_test:\\n\",\n",
    "    \"    if era == 'steroid':\\n\",\n",
    "    \"        era_df = df[(df['year'] >= 1994) & (df['year'] <= 2005)]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        era_df = df[df['year'] >= 2006]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Prepare data\\n\",\n",
    "    \"    era_test = era_df[reg_features + ['wins']].dropna()\\n\",\n",
    "    \"    if len(era_test) > 0:\\n\",\n",
    "    \"        X_era = era_test[reg_features]\\n\",\n",
    "    \"        y_era = era_test['wins']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Predict\\n\",\n",
    "    \"        era_pred = wins_predictor.predict(X_era)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calculate metrics\\n\",\n",
    "    \"        from sklearn.metrics import mean_squared_error, r2_score\\n\",\n",
    "    \"        rmse = np.sqrt(mean_squared_error(y_era, era_pred))\\n\",\n",
    "    \"        r2 = r2_score(y_era, era_pred)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        era_results.append({\\n\",\n",
    "    \"            'Era': era,\\n\",\n",
    "    \"            'Samples': len(era_test),\\n\",\n",
    "    \"            'RMSE': rmse,\\n\",\n",
    "    \"            'R²': r2,\\n\",\n",
    "    \"            'MAE': np.mean(np.abs(era_pred - y_era))\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"\\n\",\n",
    "    \"era_results_df = pd.DataFrame(era_results)\\n\",\n",
    "    \"print(\\\"\\\\nModel Performance by Era:\\\")\\n\",\n",
    "    \"print(era_results_df.round(3))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Error Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze systematic biases\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(12, 10))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Error vs actual wins\\n\",\n",
    "    \"ax = axes[0, 0]\\n\",\n",
    "    \"ax.scatter(reg_analysis['actual_wins'], reg_analysis['error'], alpha=0.5)\\n\",\n",
    "    \"ax.axhline(0, color='red', linestyle='--')\\n\",\n",
    "    \"ax.set_xlabel('Actual Wins')\\n\",\n",
    "    \"ax.set_ylabel('Prediction Error')\\n\",\n",
    "    \"ax.set_title('Error vs Actual Wins')\\n\",\n",
    "    \"ax.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Error distribution by team\\n\",\n",
    "    \"ax = axes[0, 1]\\n\",\n",
    "    \"team_errors = reg_analysis.groupby('team')['abs_error'].mean().sort_values(ascending=False).head(15)\\n\",\n",
    "    \"team_errors.plot(kind='barh', ax=ax)\\n\",\n",
    "    \"ax.set_xlabel('Mean Absolute Error')\\n\",\n",
    "    \"ax.set_title('Prediction Error by Team (Top 15)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Error vs predicted wins\\n\",\n",
    "    \"ax = axes[1, 0]\\n\",\n",
    "    \"ax.scatter(reg_analysis['predicted_wins'], reg_analysis['error'], alpha=0.5)\\n\",\n",
    "    \"ax.axhline(0, color='red', linestyle='--')\\n\",\n",
    "    \"ax.set_xlabel('Predicted Wins')\\n\",\n",
    "    \"ax.set_ylabel('Prediction Error')\\n\",\n",
    "    \"ax.set_title('Error vs Predicted Wins')\\n\",\n",
    "    \"ax.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Residual normality\\n\",\n",
    "    \"ax = axes[1, 1]\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"stats.probplot(reg_analysis['error'], dist=\\\"norm\\\", plot=ax)\\n\",\n",
    "    \"ax.set_title('Q-Q Plot of Residuals')\\n\",\n",
    "    \"ax.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Model Comparison Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create comprehensive evaluation report\\n\",\n",
    "    \"evaluation_report = {\\n\",\n",
    "    \"    'classification': {\\n\",\n",
    "    \"        'model_type': 'Division Winner Classifier',\\n\",\n",
    "    \"        'test_samples': len(y_test_class),\\n\",\n",
    "    \"        'accuracy': metrics['accuracy'],\\n\",\n",
    "    \"        'precision': metrics['precision'],\\n\",\n",
    "    \"        'recall': metrics['recall'],\\n\",\n",
    "    \"        'f1_score': metrics['f1'],\\n\",\n",
    "    \"        'roc_auc': metrics.get('roc_auc', 'N/A'),\\n\",\n",
    "    \"        'confidence_accuracy': conf_accuracy.to_dict()\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'regression': {\\n\",\n",
    "    \"        'model_type': 'Wins Predictor',\\n\",\n",
    "    \"        'test_samples': len(y_test_reg),\\n\",\n",
    "    \"        'rmse': metrics_reg['rmse'],\\n\",\n",
    "    \"        'mae': metrics_reg['mae'],\\n\",\n",
    "    \"        'r2': metrics_reg['r2'],\\n\",\n",
    "    \"        'within_5_wins': metrics_reg['within_5'],\\n\",\n",
    "    \"        'within_10_wins': metrics_reg['within_10'],\\n\",\n",
    "    \"        'mean_error': metrics_reg['mean_residual'],\\n\",\n",
    "    \"        'std_error': metrics_reg['std_residual']\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'evaluation_date': pd.Timestamp.now().isoformat()\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save evaluation report\\n\",\n",
    "    \"report_path = Path('../models/evaluation_report.json')\\n\",\n",
    "    \"with open(report_path, 'w') as f:\\n\",\n",
    "    \"    json.dump(evaluation_report, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nEvaluation Summary:\\\")\\n\",\n",
    "    \"print(\\\"\\\\nClassification Model:\\\")\\n\",\n",
    "    \"for key, value in evaluation_report['classification'].items():\\n\",\n",
    "    \"    if key not in ['confidence_accuracy', 'model_type', 'test_samples']:\\n\",\n",
    "    \"        print(f\\\"  {key}: {value:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nRegression Model:\\\")\\n\",\n",
    "    \"for key, value in evaluation_report['regression'].items():\\n\",\n",
    "    \"    if key not in ['model_type', 'test_samples']:\\n\",\n",
    "    \"        if isinstance(value, float):\\n\",\n",
    "    \"            if key in ['rmse', 'mae', 'mean_error', 'std_error']:\\n\",\n",
    "    \"                print(f\\\"  {key}: {value:.2f}\\\")\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                print(f\\\"  {key}: {value:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Generate Final Visualizations\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create evaluation dashboard\\n\",\n",
    "    \"evaluator = ClassificationEvaluator(division_predictor.model, 'Division Winner Classifier')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate comprehensive report\\n\",\n",
    "    \"eval_results = evaluator.evaluate(\\n\",\n",
    "    \"    X_test_class.values, \\n\",\n",
    "    \"    y_test_class.values,\\n\",\n",
    "    \"    feature_names=class_features\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save evaluation plots\\n\",\n",
    "    \"evaluator.save_results(include_plots=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nEvaluation complete! Results and plots saved to evaluation_results/\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Conclusions\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Findings:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Classification Model Performance**:\\n\",\n",
    "    \"   - The division winner classifier achieves good accuracy with reasonable precision/recall balance\\n\",\n",
    "    \"   - Higher confidence predictions show better accuracy\\n\",\n",
    "    \"   - Model is well-calibrated for probability estimates\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. **Regression Model Performance**:\\n\",\n",
    "    \"   - Win predictions are accurate within acceptable margins\\n\",\n",
    "    \"   - Most predictions fall within 5-10 wins of actual results\\n\",\n",
    "    \"   - No significant systematic bias detected\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. **Areas for Improvement**:\\n\",\n",
    "    \"   - Some teams are consistently harder to predict\\n\",\n",
    "    \"   - Era effects suggest potential for era-specific models\\n\",\n",
    "    \"   - Feature engineering could focus on team-specific patterns\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Next Steps:\\n\",\n",
    "    \"- Deploy models for production use\\n\",\n",
    "    \"- Set up monitoring for model drift\\n\",\n",
    "    \"- Schedule regular retraining with new data\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
